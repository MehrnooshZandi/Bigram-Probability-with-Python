# Bigram-Probability-with-Python
In this Repository we calculate bigram probability with Python.
Bigrams help provide the conditional probability of a token given the preceding token, when the relation of the conditional probability is applied:

P(W_{n}|W_{{n-1}})={P(W_{{n-1}},W_{n}) \P(W_{{n-1}})

That is, the probability P() of a token W_{n} given the preceding token W_{n-1} is equal to the probability of their bigram, or the co-occurrence of the two tokens P(W_{{n-1}},W_{n}), divided by the probability of the preceding token.

Bigrams are used in most successful language models for speech recognition. They are a special case of N-gram.

Bigram frequency attacks can be used in cryptography to solve cryptograms. See frequency analysis.

Bigram frequency is one approach to statistical language identification.

Some activities in logology or recreational linguistics involve bigrams. These include attempts to find English words beginning with every possible bigram, or words containing a string of repeated bigrams, such as logogogue.
